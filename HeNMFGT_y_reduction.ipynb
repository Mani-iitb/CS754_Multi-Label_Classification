{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f611d688",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2493a812",
      "metadata": {},
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "92271213",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92271213",
        "outputId": "e75c97df-888c-475c-8bac-1d77ee533412"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix, hstack, vstack, find as sparse_find, issparse\n",
        "import community as community_louvain\n",
        "import matplotlib.pyplot as plt\n",
        "from community.community_louvain import best_partition\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import os\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.cluster import SpectralClustering\n",
        "import scipy.sparse.linalg as splinalg\n",
        "import random\n",
        "from collections import defaultdict, deque\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import hinge_loss\n",
        "import scipy.sparse as sp\n",
        "from networkx.algorithms.community import kernighan_lin_bisection\n",
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c7fa42",
      "metadata": {
        "id": "33c7fa42"
      },
      "source": [
        "DataLoader: The function to load data.<br>\n",
        "input: filepath (string)<br>\n",
        "output: X and Y data in the shape of (csr_matrix n_samples x n_features) and (csr_matrix n_samples x n_labels) respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f70dd9",
      "metadata": {
        "id": "28f70dd9"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def load_multilabel_data(self, filepath):\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Error: Data file not found at {filepath}\")\n",
        "            return None, None\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                first_line = f.readline().split()\n",
        "                if len(first_line) < 3:\n",
        "                    raise ValueError(\"Invalid header format in data file.\")\n",
        "                n_samples = int(first_line[0])\n",
        "                n_features = int(first_line[1])\n",
        "                n_labels = int(first_line[2])\n",
        "\n",
        "                if n_samples <= 0 or n_features <= 0 or n_labels <= 0:\n",
        "                    raise ValueError(\"Invalid dimensions in header.\")\n",
        "\n",
        "                X_lil = lil_matrix((n_samples, n_features), dtype=np.float64)\n",
        "                Y_lil = lil_matrix((n_samples, n_labels), dtype=np.int8)\n",
        "\n",
        "                for i, line in enumerate(f):\n",
        "                    if i >= n_samples:\n",
        "                        print(f\"Warning: More lines than expected ({n_samples}) in data file. Stopping.\")\n",
        "                        break\n",
        "\n",
        "                    parts = line.strip().split(' ', 1)\n",
        "                    if ':' in parts[0]:\n",
        "                        labels_str = \"\"\n",
        "                        features_str = parts[0] + \" \" + parts[1]\n",
        "                    else:\n",
        "                        labels_str = parts[0]\n",
        "                        features_str = parts[1] if len(parts) > 1 else ''\n",
        "\n",
        "                    if labels_str:\n",
        "                        try:\n",
        "                            label_indices = [int(l) for l in labels_str.split(',')]\n",
        "                            for label_idx in label_indices:\n",
        "                                if 0 <= label_idx < n_labels:\n",
        "                                    Y_lil[i, label_idx] = 1\n",
        "                                else:\n",
        "                                    print(f\"Warning: Label index {label_idx} out of bounds [0, {n_labels-1}) in line {i+2}. Skipping.\")\n",
        "                        except ValueError:\n",
        "                            print(f\"Warning: Skipping incorrect label string '{labels_str}' in line {i+2}\")\n",
        "\n",
        "                    if features_str:\n",
        "                        for feature_pair in features_str.split(' '):\n",
        "                            try:\n",
        "                                idx_str, val_str = feature_pair.split(':')\n",
        "                                feature_idx = int(idx_str)\n",
        "                                feature_val = float(val_str)\n",
        "                                if 0 <= feature_idx < n_features:\n",
        "                                    X_lil[i, feature_idx] = feature_val\n",
        "                                else:\n",
        "                                    print(f\"Warning: Feature index {feature_idx} out of bounds [0, {n_features-1}) in line {i+2}. Skipping.\")\n",
        "                            except ValueError:\n",
        "                                print(f\"Warning: Skipping incorrect feature pair '{feature_pair}' in line {i+2}\")\n",
        "                                continue\n",
        "\n",
        "            X_csr = X_lil.tocsr()\n",
        "            Y_csr = Y_lil.tocsr()\n",
        "            print(f\"Loaded data: X shape {X_csr.shape}, Y shape {Y_csr.shape}\")\n",
        "            print(f\"X sparsity: {X_csr.nnz / (X_csr.shape[0] * X_csr.shape[1]):.4f}\")\n",
        "            print(f\"Y sparsity: {Y_csr.nnz / (Y_csr.shape[0] * Y_csr.shape[1]):.4f}\")\n",
        "            return X_csr, Y_csr\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from {filepath}: {e}\")\n",
        "            return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5defb663",
      "metadata": {},
      "source": [
        "### Loading the datasets using the dataloader defined above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f8bc1b1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8bc1b1d",
        "outputId": "90f780ce-2e25-4aef-f1de-da64b9de2467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data: X shape (15539, 5000), Y shape (15539, 3993)\n",
            "X sparsity: 0.0474\n",
            "Y sparsity: 0.0013\n",
            "Loaded data: X shape (3809, 5000), Y shape (3809, 3993)\n",
            "X sparsity: 0.0474\n",
            "Y sparsity: 0.0013\n"
          ]
        }
      ],
      "source": [
        "data_filepath_train = 'Eurlex/eurlex_train.txt'\n",
        "data_filepath_test = 'Eurlex/eurlex_test.txt'\n",
        "data_loader = DataLoader()\n",
        "X_train, Y_train = data_loader.load_multilabel_data(\n",
        "    data_filepath_train\n",
        ")\n",
        "X_test, Y_test = data_loader.load_multilabel_data(\n",
        "    data_filepath_test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc151718",
      "metadata": {},
      "source": [
        "### Importing the required modules for the implementation of autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f68d780",
      "metadata": {
        "id": "9f68d780"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ea9ee154",
      "metadata": {
        "id": "ea9ee154"
      },
      "outputs": [],
      "source": [
        "Y = np.concatenate((Y_train.toarray(), Y_train.toarray()), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546f9fb2",
      "metadata": {
        "id": "546f9fb2"
      },
      "outputs": [],
      "source": [
        "mean = Y.mean(axis=0)\n",
        "std = Y.std(axis=0)\n",
        "\n",
        "# Avoid divide-by-zero: replace 0 stds with 1 (neutral scaling)\n",
        "std[std == 0] = 1.0\n",
        "Y = (Y - mean) / std\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(Y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3b4ec3",
      "metadata": {},
      "source": [
        "### Auto encoder used to reduce the dimensionality of the Y data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1977532f",
      "metadata": {
        "id": "1977532f"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=5000, encoding_dim=1024):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69828e39",
      "metadata": {},
      "source": [
        "### Training the autoencoder to reduce the dimensionality of the Y data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "de13214d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de13214d",
        "outputId": "c5ce5776-acf4-4a5c-a79a-57d861ca8bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 462.2344\n",
            "Epoch 2, Loss: 461.7642\n",
            "Epoch 3, Loss: 461.8189\n",
            "Epoch 4, Loss: 461.7678\n",
            "Epoch 5, Loss: 461.8166\n",
            "Epoch 6, Loss: 461.7624\n",
            "Epoch 7, Loss: 461.7850\n",
            "Epoch 8, Loss: 461.8508\n",
            "Epoch 9, Loss: 461.7073\n",
            "Epoch 10, Loss: 461.7911\n"
          ]
        }
      ],
      "source": [
        "model = Autoencoder(input_dim=Y.shape[1]).to('cuda')\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        x_batch = batch[0]\n",
        "        optimizer.zero_grad()\n",
        "        x_batch = x_batch.to('cuda')\n",
        "        output = model(x_batch)\n",
        "        loss = criterion(output, x_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba828921",
      "metadata": {},
      "source": [
        "### reducing the dimensionality of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a657f307",
      "metadata": {
        "id": "a657f307"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    Y_train_reduce = model.encoder(torch.tensor(Y_train.toarray(), dtype=torch.float32).to('cuda')).cpu().numpy()\n",
        "    Y_test_reduce = model.encoder(torch.tensor(Y_test.toarray(), dtype=torch.float32).to('cuda')).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "a7f50eec",
      "metadata": {
        "id": "a7f50eec"
      },
      "outputs": [],
      "source": [
        "Y_train = csr_matrix(Y_train_reduce)\n",
        "Y_test = csr_matrix(Y_test_reduce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d9640e9f",
      "metadata": {
        "id": "d9640e9f"
      },
      "outputs": [],
      "source": [
        "Y_train_T = Y_train.T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e37ea4e",
      "metadata": {
        "id": "9e37ea4e"
      },
      "source": [
        "### Function to partition the co-occurance matrix hierarchically. <br>\n",
        "### Input : Y matrix, depth which is used for partitioning <br>\n",
        "### output : permutation of the labels and partitioned indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "40136ccc",
      "metadata": {
        "id": "40136ccc"
      },
      "outputs": [],
      "source": [
        "def hierarchical_partition(Y, depth=3):\n",
        "    A = Y @ Y.T\n",
        "    G = nx.from_scipy_sparse_array(sp.csr_matrix(A))\n",
        "\n",
        "    def recursive_partition(G, depth):\n",
        "        if depth == 0 or G.number_of_nodes() <= 1:\n",
        "            return [list(G.nodes)]\n",
        "        try:\n",
        "            part1, part2 = kernighan_lin_bisection(G)\n",
        "        except:\n",
        "            return [list(G.nodes)]\n",
        "        G1 = G.subgraph(part1)\n",
        "        G2 = G.subgraph(part2)\n",
        "        return recursive_partition(G1, depth - 1) + recursive_partition(G2, depth - 1)\n",
        "\n",
        "    partitions = recursive_partition(G, depth)\n",
        "    permutation = [node for group in partitions for node in group]\n",
        "\n",
        "    return permutation, partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "757aa6e9",
      "metadata": {
        "id": "757aa6e9"
      },
      "outputs": [],
      "source": [
        "result = hierarchical_partition(Y_train_T, depth=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "95b07dbd",
      "metadata": {
        "id": "95b07dbd"
      },
      "outputs": [],
      "source": [
        "permutation = result[0]\n",
        "partition = result[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ef0753",
      "metadata": {
        "id": "42ef0753"
      },
      "source": [
        "### Applying the permutations to the Y matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "bb5725e1",
      "metadata": {
        "id": "bb5725e1"
      },
      "outputs": [],
      "source": [
        "Y_train = Y_train.transpose().astype(np.int32)\n",
        "Y_test = Y_test.transpose().astype(np.int32)\n",
        "cooccurrence = (Y_train @ Y_train.T).toarray()\n",
        "cooccurrence = cooccurrence[permutation][:, permutation]\n",
        "Y_train_perm = Y_train[permutation]\n",
        "Y_test_perm = Y_test[permutation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "a9c03da1",
      "metadata": {
        "id": "a9c03da1"
      },
      "outputs": [],
      "source": [
        "set_list = partition.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025f9951",
      "metadata": {
        "id": "025f9951"
      },
      "source": [
        "### Partitioning the Y matrix based on the partition as per the hierarchical partitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "f7d79ee2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7d79ee2",
        "outputId": "329e6ff0-ac59-451a-e69e-30102df07066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "256\n",
            "256\n",
            "256\n",
            "256\n"
          ]
        }
      ],
      "source": [
        "num_partitions = len(set_list)\n",
        "\n",
        "Yp = []\n",
        "Ytp = []\n",
        "for i in range(num_partitions):\n",
        "    Yp.append(Y_train_perm[set_list[i],:])\n",
        "    Ytp.append(Y_test_perm[set_list[i],:])\n",
        "    print(len(set_list[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "cbec0057",
      "metadata": {
        "id": "cbec0057"
      },
      "outputs": [],
      "source": [
        "K_EVAL = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f311a8",
      "metadata": {
        "id": "57f311a8"
      },
      "source": [
        "### A simple predictor for cases where all labels in a group are constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "29774347",
      "metadata": {
        "id": "29774347"
      },
      "outputs": [],
      "source": [
        "class ConstantPredictor:\n",
        "    def __init__(self, value):\n",
        "        if value not in [0, 1]:\n",
        "             raise ValueError(\"Constant value must be 0 or 1\")\n",
        "        self.value = int(value)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.full(X.shape[0], self.value, dtype=np.int8)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        proba = np.zeros((n_samples, 2), dtype=float)\n",
        "        proba[:, self.value] = 1.0\n",
        "        return proba\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08df9ac7",
      "metadata": {},
      "source": [
        "### NMFGT class, which will train the NMFGT model by creating the GT matrix and training many intermediate classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e89f900",
      "metadata": {
        "id": "9e89f900"
      },
      "outputs": [],
      "source": [
        "class NMFGTModel:\n",
        "    def __init__(self, n_groups, k_target_sparsity=K_EVAL, column_sparsity_range=None,\n",
        "                nmf_options=None, classifier_options=None, random_state=42):\n",
        "        \"\"\"\n",
        "        Constructor method to initialize the NMFGTModel.\n",
        "        Args:\n",
        "            n_groups (int): Number of groups (m), the target dimension for reduction\n",
        "            k_target_sparsity (int): Target label sparsity for evaluation and c selection\n",
        "            column_sparsity_range (list or range): Range of column sparsity values (c) to test\n",
        "            nmf_options (dict, optional): Options for scikit-learn's NMF\n",
        "            classifier_options (dict, optional): Options for scikit-learn's LogisticRegression\n",
        "            random_state (int): Random seed for reproducing results\n",
        "        \"\"\"\n",
        "        self.n_groups = n_groups\n",
        "        self.k_target_sparsity = k_target_sparsity\n",
        "        self.column_sparsity_range = column_sparsity_range if column_sparsity_range is not None else list(range(10, 71, 10))\n",
        "        self.random_state = random_state\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "        self.nmf_options = {\n",
        "            'init': 'nndsvda', 'max_iter': 1500, 'tol': 1e-4,\n",
        "            'solver': 'cd', 'beta_loss': 'frobenius', 'random_state': random_state,\n",
        "            'n_components': self.n_groups\n",
        "        }\n",
        "\n",
        "        if nmf_options:\n",
        "            self.nmf_options.update(nmf_options)\n",
        "            self.nmf_options['n_components'] = self.n_groups\n",
        "\n",
        "        self.classifier_options = {\n",
        "            'penalty':'l2',\n",
        "            'loss':'squared_hinge',\n",
        "            'max_iter':1000,\n",
        "            'dual':True,\n",
        "            'C':1.0,\n",
        "            'random_state':random_state\n",
        "        }\n",
        "\n",
        "        if classifier_options:\n",
        "            self.classifier_options.update(classifier_options)\n",
        "            self.classifier_options['random_state'] = random_state\n",
        "\n",
        "\n",
        "        self.gt_matrix_ = None\n",
        "        self.selected_column_sparsity_ = None\n",
        "        self.nmf_reconstruction_error_ = None \n",
        "        self.selection_hamming_loss_ = None \n",
        "        self.classifiers_ = None \n",
        "        self.train_time_ = None\n",
        "\n",
        "    def _reweight_proba_vector(self, P_initial, column_sparsity):\n",
        "        \"\"\"Reweights a probability vector.\"\"\"\n",
        "\n",
        "        n = len(P_initial)\n",
        "        Pi_weighted = column_sparsity * np.array(P_initial)\n",
        "        id1_mask = Pi_weighted >= 1\n",
        "\n",
        "        if np.sum(id1_mask) >= column_sparsity:\n",
        "            Pi_weighted[id1_mask] = 1\n",
        "            return Pi_weighted\n",
        "\n",
        "        id2_sorted_indices = np.argsort(Pi_weighted)[::-1]\n",
        "        tmp = Pi_weighted[id2_sorted_indices]\n",
        "\n",
        "        exc = 0.0\n",
        "        for j in range(n):\n",
        "            if tmp[j] >= 1.0:\n",
        "                exc += (tmp[j] - 1.0)\n",
        "                tmp[j] = 1.0\n",
        "            elif exc > 1e-9:\n",
        "                sum_remaining = np.sum(tmp[j:])\n",
        "                if sum_remaining > 1e-9:\n",
        "                    factor = 1.0 + exc / sum_remaining\n",
        "                    tmp[j:] *= factor\n",
        "                    exc = 0.0\n",
        "                else:\n",
        "                    remaining_count = n - j\n",
        "                    if remaining_count > 0:\n",
        "                        add_val = exc / remaining_count\n",
        "                        tmp[j:] += add_val\n",
        "                        exc = 0.0\n",
        "                became_one_mask = tmp[j:] >= 1.0\n",
        "                if np.any(became_one_mask):\n",
        "                    exc += np.sum(tmp[j:][became_one_mask] - 1.0)\n",
        "                    tmp[j:][became_one_mask] = 1.0\n",
        "\n",
        "            if np.sum(tmp >= 1.0) >= column_sparsity or exc < 1e-9:\n",
        "                Pi_final = np.zeros_like(Pi_weighted)\n",
        "                Pi_final[id2_sorted_indices] = tmp\n",
        "                Pi_final[Pi_final < 0] = 0\n",
        "                return Pi_final\n",
        "\n",
        "        Pi_final = np.zeros_like(Pi_weighted)\n",
        "        Pi_final[id2_sorted_indices] = tmp\n",
        "        Pi_final[Pi_final < 0] = 0\n",
        "        return Pi_final\n",
        "\n",
        "    def _build_sparse_rand_vector(self, P_initial, column_sparsity):\n",
        "        \"\"\"Generates a sparse random vector based on probabilities.\"\"\"\n",
        "        n = len(P_initial)\n",
        "        P_reweighted = self._reweight_proba_vector(P_initial, column_sparsity)\n",
        "        vec = np.zeros(n, dtype=np.int8)\n",
        "        rand_nums = np.random.rand(n)\n",
        "        vec[P_reweighted >= 1.0] = 1\n",
        "        sample_mask = (P_reweighted < 1.0) & (P_reweighted > 0)\n",
        "        vec[sample_mask] = (rand_nums[sample_mask] < P_reweighted[sample_mask]).astype(np.int8)\n",
        "        return csr_matrix(vec).T\n",
        "\n",
        "    def _build_gt_matrix(self, H_basis, column_sparsity):\n",
        "        \"\"\"Generates the full GT matrix A.\"\"\"\n",
        "        d, m = H_basis.shape # H_basis --> (d x m)\n",
        "        if m != self.n_groups:\n",
        "            print(f\"Warning: NMF basis dimension ({m}) differs from n_groups ({self.n_groups}). Using {m}.\")\n",
        "        A_cols = []\n",
        "        print(f\"Generating GT matrix A ({m} x {d}) with target column sparsity c={column_sparsity}...\")\n",
        "        for i in range(d):\n",
        "            H_row_i = H_basis[i, :]\n",
        "            sum_H_row_i = np.sum(H_row_i)\n",
        "            if sum_H_row_i > 1e-9:\n",
        "                P_initial = H_row_i / sum_H_row_i\n",
        "            else:\n",
        "                P_initial = np.ones(m) / m\n",
        "            A_col_i = self._build_sparse_rand_vector(P_initial, column_sparsity)\n",
        "            A_cols.append(A_col_i)\n",
        "            if (i + 1) % 50 == 0 or (i + 1) == d:\n",
        "                print(f\"Generated GT column {i+1}/{d}\")\n",
        "        A = hstack(A_cols, format='csc')\n",
        "        print(f\"Generated GT matrix A: shape {A.shape}, nnz {A.nnz}, avg col sparsity {A.nnz / d:.2f}\")\n",
        "        return A\n",
        "\n",
        "    def _build_A(self, Y_train_csc):\n",
        "        \"\"\"\n",
        "        Performs NMF, selects best 'c', and generates the GT matrix A.\n",
        "        \"\"\"\n",
        "        d, n_train = Y_train_csc.shape\n",
        "        print(f\"\\nSelecting best column sparsity 'c' from {self.column_sparsity_range}...\")\n",
        "        print(\"Calculating YYT (may take time)...\")\n",
        "\n",
        "        start_yyt = time.time()\n",
        "        try:\n",
        "            YYT = Y_train_csc @ Y_train_csc.T   # (d x d)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating YYT: {e}.\")\n",
        "            return None, None, None\n",
        "        end_yyt = time.time()\n",
        "\n",
        "        print(f\"YYT calculation took {end_yyt - start_yyt:.2f}s. Shape: {YYT.shape}\")\n",
        "\n",
        "        if issparse(YYT):\n",
        "            neg_indices = YYT < 0\n",
        "            if neg_indices.nnz > 0:\n",
        "                print(f\"Warning: Found {neg_indices.nnz} negative values in sparse YYT. Clamping to 0.\")\n",
        "                YYT[neg_indices] = 0\n",
        "        else:\n",
        "            YYT[YYT < 0] = 0\n",
        "\n",
        "\n",
        "        print(f\"Performing NMF on YY^T (n_components={self.n_groups})...\")\n",
        "        start_nmf = time.time()\n",
        "        nmf_model = NMF(**self.nmf_options)\n",
        "        try:\n",
        "            H_basis = nmf_model.fit_transform(YYT)\n",
        "            self.nmf_reconstruction_error_ = nmf_model.reconstruction_err_\n",
        "\n",
        "        except TypeError:\n",
        "            print(\"NMF requires dense input, converting clamped YYT...\")\n",
        "            if YYT.nnz > 500e6:\n",
        "                print(\"Warning: YYT is very large\")\n",
        "            YYT_dense = YYT.toarray()\n",
        "\n",
        "            H_basis = nmf_model.fit_transform(YYT_dense)\n",
        "            self.nmf_reconstruction_error_ = nmf_model.reconstruction_err_\n",
        "            del YYT_dense\n",
        "\n",
        "        except ValueError as ve:\n",
        "            if \"Negative values in data passed\" in str(ve):\n",
        "                print(\"Error: Negative values persist even after clamping. Check YYT calculation or NMF internal steps.\")\n",
        "            else:\n",
        "                print(f\"ValueError during NMF: {ve}\")\n",
        "            del YYT\n",
        "            return None, None, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during NMF: {e}\")\n",
        "            del YYT\n",
        "            return None, None, None\n",
        "\n",
        "        end_nmf = time.time()\n",
        "        print(f\"NMF completed in {end_nmf - start_nmf:.2f}s. Basis shape: {H_basis.shape}, Rec. Error: {self.nmf_reconstruction_error_:.4f}\")\n",
        "        del YYT\n",
        "\n",
        "        min_avg_hamming_loss = float('inf')\n",
        "        best_c = -1\n",
        "        generated_matrices = {}\n",
        "\n",
        "        n_eval = min(n_train, 500)\n",
        "        Y_eval_csc = Y_train_csc[:, :n_eval]\n",
        "\n",
        "        for c_val in self.column_sparsity_range:\n",
        "            print(f\"\\n  Testing c = {c_val}...\")\n",
        "            A_candidate = self._build_gt_matrix(H_basis, c_val) # m x d\n",
        "            if A_candidate is None:\n",
        "                continue\n",
        "\n",
        "            # We evaluate Hamming Loss on subset Y_eval_csc (d x n_eval)\n",
        "            Z_reduced_eval = (A_candidate @ Y_eval_csc > 0).astype(np.int8) # m x n_eval\n",
        "            Scores_rec_eval = A_candidate.T @ Z_reduced_eval # d x n_eval\n",
        "\n",
        "            total_hamming_dist = 0\n",
        "            for l in range(n_eval):\n",
        "                scores_l = Scores_rec_eval[:, l].toarray().ravel()\n",
        "                y_true_l = Y_eval_csc[:, l].toarray().ravel()\n",
        "                k_actual = int(max(1, np.sum(y_true_l)))\n",
        "                k_predict = min(k_actual, self.k_target_sparsity)\n",
        "\n",
        "                if len(scores_l) == 0 or np.all(scores_l == scores_l[0]):\n",
        "                    top_k_indices = np.random.choice(d, size=min(k_predict, d), replace=False)\n",
        "                else:\n",
        "                    top_k_indices = np.argsort(scores_l)[::-1][:k_predict]\n",
        "\n",
        "                y_pred_l = np.zeros(d, dtype=np.int8)\n",
        "                if len(top_k_indices) > 0:\n",
        "                    y_pred_l[top_k_indices] = 1\n",
        "                total_hamming_dist += np.sum(y_pred_l != y_true_l)\n",
        "\n",
        "            avg_hamming_loss = total_hamming_dist / (n_eval * d)\n",
        "            generated_matrices[c_val] = A_candidate\n",
        "            print(f\"Avg Hamming Loss (on {n_eval} samples): {avg_hamming_loss:.6f}\")\n",
        "\n",
        "            if avg_hamming_loss < min_avg_hamming_loss:\n",
        "                min_avg_hamming_loss = avg_hamming_loss\n",
        "                best_c = c_val\n",
        "\n",
        "        if best_c == -1 and generated_matrices:\n",
        "            best_c = self.column_sparsity_range[0]\n",
        "            min_avg_hamming_loss = np.inf\n",
        "\n",
        "        print(f\"\\nSelected c = {best_c} with min Hamming Loss = {min_avg_hamming_loss:.6f}\")\n",
        "        best_A = generated_matrices.get(best_c)\n",
        "        if best_A is None:\n",
        "            print(\"Error: Could not retrieve the generated GT matrix for the best c\")\n",
        "            return None, None, None\n",
        "\n",
        "        self.selection_hamming_loss_ = min_avg_hamming_loss\n",
        "        return best_A, best_c, min_avg_hamming_loss\n",
        "\n",
        "    def fit(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        Trains the NMF-GT model.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"\\n--- Starting NMF-GT Model Training ---\")\n",
        "        start_fit_time = time.time()\n",
        "        n_samples, n_features = X_train.shape\n",
        "        d = Y_train.shape[1]\n",
        "\n",
        "        if Y_train.shape[0] != n_samples:\n",
        "            raise ValueError(\"X_train and Y_train must have the same number of samples.\")\n",
        "\n",
        "        Y_train_csc_T = Y_train.T.tocsc()\n",
        "        best_A, best_c, _ = self._build_A(Y_train_csc_T)\n",
        "        return best_A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "444ac5f4",
      "metadata": {
        "id": "444ac5f4"
      },
      "source": [
        "### Initialization of the variables for the NMFGT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c48a1485",
      "metadata": {
        "id": "c48a1485"
      },
      "outputs": [],
      "source": [
        "n_groups_m = 100\n",
        "k_target = 5\n",
        "c_range = list(range(5, 96, 5))\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea94380",
      "metadata": {
        "id": "4ea94380"
      },
      "source": [
        "### Calculating the Grouping matrix for each partition and trains a simple linear model for each partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "u5ivCzTmXe3e",
      "metadata": {
        "id": "u5ivCzTmXe3e"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c174775a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c174775a",
        "outputId": "76efefe3-ecdb-4861-b51f-6e507018499f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15539, 5000)\n",
            "(15539, 256)\n",
            "\n",
            "--- Starting NMF-GT Model Training ---\n",
            "\n",
            "Selecting best column sparsity 'c' from [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]...\n",
            "Calculating YYT (may take time)...\n",
            "YYT calculation took 0.50s. Shape: (256, 256)\n",
            "Performing NMF on YY^T (n_components=100)...\n",
            "NMF completed in 0.12s. Basis shape: (256, 100), Rec. Error: 47295.7459\n",
            "\n",
            "  Testing c = 5...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=5...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 1236, avg col sparsity 4.83\n",
            "Avg Hamming Loss (on 500 samples): 0.285172\n",
            "\n",
            "  Testing c = 10...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=10...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 2491, avg col sparsity 9.73\n",
            "Avg Hamming Loss (on 500 samples): 0.295914\n",
            "\n",
            "  Testing c = 15...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=15...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 3884, avg col sparsity 15.17\n",
            "Avg Hamming Loss (on 500 samples): 0.292203\n",
            "\n",
            "  Testing c = 20...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=20...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 5186, avg col sparsity 20.26\n",
            "Avg Hamming Loss (on 500 samples): 0.288945\n",
            "\n",
            "  Testing c = 25...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=25...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 6277, avg col sparsity 24.52\n",
            "Avg Hamming Loss (on 500 samples): 0.288633\n",
            "\n",
            "  Testing c = 30...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=30...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 7836, avg col sparsity 30.61\n",
            "Avg Hamming Loss (on 500 samples): 0.292312\n",
            "\n",
            "  Testing c = 35...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=35...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 8968, avg col sparsity 35.03\n",
            "Avg Hamming Loss (on 500 samples): 0.282828\n",
            "\n",
            "  Testing c = 40...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=40...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 10175, avg col sparsity 39.75\n",
            "Avg Hamming Loss (on 500 samples): 0.281273\n",
            "\n",
            "  Testing c = 45...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=45...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 11659, avg col sparsity 45.54\n",
            "Avg Hamming Loss (on 500 samples): 0.288750\n",
            "\n",
            "  Testing c = 50...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=50...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 12807, avg col sparsity 50.03\n",
            "Avg Hamming Loss (on 500 samples): 0.294320\n",
            "\n",
            "  Testing c = 55...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=55...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 14087, avg col sparsity 55.03\n",
            "Avg Hamming Loss (on 500 samples): 0.288391\n",
            "\n",
            "  Testing c = 60...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=60...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 15410, avg col sparsity 60.20\n",
            "Avg Hamming Loss (on 500 samples): 0.292016\n",
            "\n",
            "  Testing c = 65...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=65...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 16741, avg col sparsity 65.39\n",
            "Avg Hamming Loss (on 500 samples): 0.289797\n",
            "\n",
            "  Testing c = 70...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=70...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 17989, avg col sparsity 70.27\n",
            "Avg Hamming Loss (on 500 samples): 0.294734\n",
            "\n",
            "  Testing c = 75...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=75...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 19084, avg col sparsity 74.55\n",
            "Avg Hamming Loss (on 500 samples): 0.291484\n",
            "\n",
            "  Testing c = 80...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=80...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 20478, avg col sparsity 79.99\n",
            "Avg Hamming Loss (on 500 samples): 0.283367\n",
            "\n",
            "  Testing c = 85...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=85...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 21828, avg col sparsity 85.27\n",
            "Avg Hamming Loss (on 500 samples): 0.280805\n",
            "\n",
            "  Testing c = 90...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=90...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 23026, avg col sparsity 89.95\n",
            "Avg Hamming Loss (on 500 samples): 0.288125\n",
            "\n",
            "  Testing c = 95...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=95...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 24285, avg col sparsity 94.86\n",
            "Avg Hamming Loss (on 500 samples): 0.295602\n",
            "\n",
            "Selected c = 85 with min Hamming Loss = 0.280805\n",
            "(15539, 256)\n",
            "(15539, 5000)\n",
            "Trained classifier 1/100 (8.9s elapsed)\n",
            "Trained classifier 2/100 (12.8s elapsed)\n",
            "Trained classifier 3/100 (18.4s elapsed)\n",
            "Trained classifier 4/100 (22.5s elapsed)\n",
            "Trained classifier 5/100 (25.8s elapsed)\n",
            "Trained classifier 6/100 (31.2s elapsed)\n",
            "Trained classifier 7/100 (36.2s elapsed)\n",
            "Trained classifier 8/100 (39.7s elapsed)\n",
            "Trained classifier 9/100 (47.2s elapsed)\n",
            "Trained classifier 10/100 (52.9s elapsed)\n",
            "Trained classifier 11/100 (58.3s elapsed)\n",
            "Trained classifier 12/100 (64.8s elapsed)\n",
            "Trained classifier 13/100 (70.4s elapsed)\n",
            "Trained classifier 14/100 (74.5s elapsed)\n",
            "Trained classifier 15/100 (78.9s elapsed)\n",
            "Trained classifier 16/100 (82.9s elapsed)\n",
            "Trained classifier 17/100 (89.3s elapsed)\n",
            "Trained classifier 18/100 (94.2s elapsed)\n",
            "Trained classifier 19/100 (101.3s elapsed)\n",
            "Trained classifier 20/100 (104.6s elapsed)\n",
            "Trained classifier 21/100 (109.0s elapsed)\n",
            "Trained classifier 22/100 (114.4s elapsed)\n",
            "Trained classifier 23/100 (119.0s elapsed)\n",
            "Trained classifier 24/100 (124.1s elapsed)\n",
            "Trained classifier 25/100 (129.0s elapsed)\n",
            "Trained classifier 26/100 (132.8s elapsed)\n",
            "Trained classifier 27/100 (137.1s elapsed)\n",
            "Trained classifier 28/100 (140.9s elapsed)\n",
            "Trained classifier 29/100 (145.2s elapsed)\n",
            "Trained classifier 30/100 (150.0s elapsed)\n",
            "Trained classifier 31/100 (157.0s elapsed)\n",
            "Trained classifier 32/100 (160.4s elapsed)\n",
            "Trained classifier 33/100 (163.3s elapsed)\n",
            "Trained classifier 34/100 (166.9s elapsed)\n",
            "Trained classifier 35/100 (171.4s elapsed)\n",
            "Trained classifier 36/100 (175.5s elapsed)\n",
            "Trained classifier 37/100 (179.4s elapsed)\n",
            "Trained classifier 38/100 (182.9s elapsed)\n",
            "Trained classifier 39/100 (191.2s elapsed)\n",
            "Trained classifier 40/100 (194.8s elapsed)\n",
            "Trained classifier 41/100 (199.2s elapsed)\n",
            "Trained classifier 42/100 (203.5s elapsed)\n",
            "Trained classifier 43/100 (209.1s elapsed)\n",
            "Trained classifier 44/100 (212.2s elapsed)\n",
            "Trained classifier 45/100 (217.0s elapsed)\n",
            "Trained classifier 46/100 (220.2s elapsed)\n",
            "Trained classifier 47/100 (224.9s elapsed)\n",
            "Trained classifier 48/100 (230.8s elapsed)\n",
            "Trained classifier 49/100 (234.9s elapsed)\n",
            "Trained classifier 50/100 (238.6s elapsed)\n",
            "Trained classifier 51/100 (248.6s elapsed)\n",
            "Trained classifier 52/100 (253.3s elapsed)\n",
            "Trained classifier 53/100 (257.8s elapsed)\n",
            "Trained classifier 54/100 (260.8s elapsed)\n",
            "Trained classifier 55/100 (264.1s elapsed)\n",
            "Trained classifier 56/100 (270.0s elapsed)\n",
            "Trained classifier 57/100 (277.3s elapsed)\n",
            "Trained classifier 58/100 (283.8s elapsed)\n",
            "Trained classifier 59/100 (287.9s elapsed)\n",
            "Trained classifier 60/100 (293.6s elapsed)\n",
            "Trained classifier 61/100 (297.0s elapsed)\n",
            "Trained classifier 62/100 (300.8s elapsed)\n",
            "Trained classifier 63/100 (304.9s elapsed)\n",
            "Trained classifier 64/100 (310.2s elapsed)\n",
            "Trained classifier 65/100 (316.2s elapsed)\n",
            "Trained classifier 66/100 (322.2s elapsed)\n",
            "Trained classifier 67/100 (330.5s elapsed)\n",
            "Trained classifier 68/100 (333.8s elapsed)\n",
            "Trained classifier 69/100 (339.4s elapsed)\n",
            "Trained classifier 70/100 (345.4s elapsed)\n",
            "Trained classifier 71/100 (349.5s elapsed)\n",
            "Trained classifier 72/100 (354.2s elapsed)\n",
            "Trained classifier 73/100 (358.0s elapsed)\n",
            "Trained classifier 74/100 (366.2s elapsed)\n",
            "Trained classifier 75/100 (369.6s elapsed)\n",
            "Trained classifier 76/100 (372.6s elapsed)\n",
            "Trained classifier 77/100 (380.8s elapsed)\n",
            "Trained classifier 78/100 (384.2s elapsed)\n",
            "Trained classifier 79/100 (389.1s elapsed)\n",
            "Trained classifier 80/100 (392.2s elapsed)\n",
            "Trained classifier 81/100 (398.1s elapsed)\n",
            "Trained classifier 82/100 (401.2s elapsed)\n",
            "Trained classifier 83/100 (407.8s elapsed)\n",
            "Trained classifier 84/100 (412.4s elapsed)\n",
            "Trained classifier 85/100 (419.2s elapsed)\n",
            "Trained classifier 86/100 (422.9s elapsed)\n",
            "Trained classifier 87/100 (428.9s elapsed)\n",
            "Trained classifier 88/100 (433.8s elapsed)\n",
            "Trained classifier 89/100 (437.6s elapsed)\n",
            "Trained classifier 90/100 (442.2s elapsed)\n",
            "Trained classifier 91/100 (445.8s elapsed)\n",
            "Trained classifier 92/100 (450.6s elapsed)\n",
            "Trained classifier 93/100 (456.5s elapsed)\n",
            "Trained classifier 94/100 (462.0s elapsed)\n",
            "Trained classifier 95/100 (466.3s elapsed)\n",
            "Trained classifier 96/100 (470.2s elapsed)\n",
            "Trained classifier 97/100 (477.8s elapsed)\n",
            "Trained classifier 98/100 (483.0s elapsed)\n",
            "Trained classifier 99/100 (487.2s elapsed)\n",
            "Trained classifier 100/100 (493.9s elapsed)\n",
            "(15539, 5000)\n",
            "(15539, 256)\n",
            "\n",
            "--- Starting NMF-GT Model Training ---\n",
            "\n",
            "Selecting best column sparsity 'c' from [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]...\n",
            "Calculating YYT (may take time)...\n",
            "YYT calculation took 0.46s. Shape: (256, 256)\n",
            "Performing NMF on YY^T (n_components=100)...\n",
            "NMF completed in 0.07s. Basis shape: (256, 100), Rec. Error: 45794.7855\n",
            "\n",
            "  Testing c = 5...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=5...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 1290, avg col sparsity 5.04\n",
            "Avg Hamming Loss (on 500 samples): 0.293148\n",
            "\n",
            "  Testing c = 10...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=10...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 2490, avg col sparsity 9.73\n",
            "Avg Hamming Loss (on 500 samples): 0.297055\n",
            "\n",
            "  Testing c = 15...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=15...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 3898, avg col sparsity 15.23\n",
            "Avg Hamming Loss (on 500 samples): 0.295047\n",
            "\n",
            "  Testing c = 20...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=20...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 5132, avg col sparsity 20.05\n",
            "Avg Hamming Loss (on 500 samples): 0.300242\n",
            "\n",
            "  Testing c = 25...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=25...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 6389, avg col sparsity 24.96\n",
            "Avg Hamming Loss (on 500 samples): 0.296578\n",
            "\n",
            "  Testing c = 30...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=30...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 7738, avg col sparsity 30.23\n",
            "Avg Hamming Loss (on 500 samples): 0.292172\n",
            "\n",
            "  Testing c = 35...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=35...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 8887, avg col sparsity 34.71\n",
            "Avg Hamming Loss (on 500 samples): 0.296227\n",
            "\n",
            "  Testing c = 40...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=40...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 10274, avg col sparsity 40.13\n",
            "Avg Hamming Loss (on 500 samples): 0.295641\n",
            "\n",
            "  Testing c = 45...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=45...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 11544, avg col sparsity 45.09\n",
            "Avg Hamming Loss (on 500 samples): 0.296148\n",
            "\n",
            "  Testing c = 50...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=50...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 12826, avg col sparsity 50.10\n",
            "Avg Hamming Loss (on 500 samples): 0.294570\n",
            "\n",
            "  Testing c = 55...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=55...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 14033, avg col sparsity 54.82\n",
            "Avg Hamming Loss (on 500 samples): 0.288922\n",
            "\n",
            "  Testing c = 60...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=60...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 15448, avg col sparsity 60.34\n",
            "Avg Hamming Loss (on 500 samples): 0.284758\n",
            "\n",
            "  Testing c = 65...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=65...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 16651, avg col sparsity 65.04\n",
            "Avg Hamming Loss (on 500 samples): 0.296141\n",
            "\n",
            "  Testing c = 70...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=70...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 17848, avg col sparsity 69.72\n",
            "Avg Hamming Loss (on 500 samples): 0.300516\n",
            "\n",
            "  Testing c = 75...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=75...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 19158, avg col sparsity 74.84\n",
            "Avg Hamming Loss (on 500 samples): 0.292250\n",
            "\n",
            "  Testing c = 80...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=80...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 20535, avg col sparsity 80.21\n",
            "Avg Hamming Loss (on 500 samples): 0.292945\n",
            "\n",
            "  Testing c = 85...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=85...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 21800, avg col sparsity 85.16\n",
            "Avg Hamming Loss (on 500 samples): 0.293328\n",
            "\n",
            "  Testing c = 90...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=90...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 23021, avg col sparsity 89.93\n",
            "Avg Hamming Loss (on 500 samples): 0.293531\n",
            "\n",
            "  Testing c = 95...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=95...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 24308, avg col sparsity 94.95\n",
            "Avg Hamming Loss (on 500 samples): 0.293500\n",
            "\n",
            "Selected c = 60 with min Hamming Loss = 0.284758\n",
            "(15539, 256)\n",
            "(15539, 5000)\n",
            "Trained classifier 1/100 (504.9s elapsed)\n",
            "Trained classifier 2/100 (509.0s elapsed)\n",
            "Trained classifier 3/100 (512.9s elapsed)\n",
            "Trained classifier 4/100 (519.4s elapsed)\n",
            "Trained classifier 5/100 (523.5s elapsed)\n",
            "Trained classifier 6/100 (531.3s elapsed)\n",
            "Trained classifier 7/100 (536.3s elapsed)\n",
            "Trained classifier 8/100 (540.7s elapsed)\n",
            "Trained classifier 9/100 (544.7s elapsed)\n",
            "Trained classifier 10/100 (549.0s elapsed)\n",
            "Trained classifier 11/100 (555.0s elapsed)\n",
            "Trained classifier 12/100 (559.7s elapsed)\n",
            "Trained classifier 13/100 (568.1s elapsed)\n",
            "Trained classifier 14/100 (573.6s elapsed)\n",
            "Trained classifier 15/100 (578.3s elapsed)\n",
            "Trained classifier 16/100 (586.2s elapsed)\n",
            "Trained classifier 17/100 (591.1s elapsed)\n",
            "Trained classifier 18/100 (597.4s elapsed)\n",
            "Trained classifier 19/100 (599.8s elapsed)\n",
            "Trained classifier 20/100 (603.2s elapsed)\n",
            "Trained classifier 21/100 (607.9s elapsed)\n",
            "Trained classifier 22/100 (611.6s elapsed)\n",
            "Trained classifier 23/100 (616.3s elapsed)\n",
            "Trained classifier 24/100 (619.8s elapsed)\n",
            "Trained classifier 25/100 (626.3s elapsed)\n",
            "Trained classifier 26/100 (628.9s elapsed)\n",
            "Trained classifier 27/100 (633.1s elapsed)\n",
            "Trained classifier 28/100 (641.6s elapsed)\n",
            "Trained classifier 29/100 (645.9s elapsed)\n",
            "Trained classifier 30/100 (653.2s elapsed)\n",
            "Trained classifier 31/100 (657.0s elapsed)\n",
            "Trained classifier 32/100 (664.4s elapsed)\n",
            "Trained classifier 33/100 (669.9s elapsed)\n",
            "Trained classifier 34/100 (674.7s elapsed)\n",
            "Trained classifier 35/100 (678.4s elapsed)\n",
            "Trained classifier 36/100 (688.0s elapsed)\n",
            "Trained classifier 37/100 (691.7s elapsed)\n",
            "Trained classifier 38/100 (695.6s elapsed)\n",
            "Trained classifier 39/100 (698.7s elapsed)\n",
            "Trained classifier 40/100 (702.0s elapsed)\n",
            "Trained classifier 41/100 (705.7s elapsed)\n",
            "Trained classifier 42/100 (711.8s elapsed)\n",
            "Trained classifier 43/100 (716.7s elapsed)\n",
            "Trained classifier 44/100 (720.1s elapsed)\n",
            "Trained classifier 45/100 (724.9s elapsed)\n",
            "Trained classifier 46/100 (729.5s elapsed)\n",
            "Trained classifier 47/100 (733.1s elapsed)\n",
            "Trained classifier 48/100 (738.2s elapsed)\n",
            "Trained classifier 49/100 (742.8s elapsed)\n",
            "Trained classifier 50/100 (746.1s elapsed)\n",
            "Trained classifier 51/100 (752.2s elapsed)\n",
            "Trained classifier 52/100 (756.0s elapsed)\n",
            "Trained classifier 53/100 (760.7s elapsed)\n",
            "Trained classifier 54/100 (765.5s elapsed)\n",
            "Trained classifier 55/100 (769.6s elapsed)\n",
            "Trained classifier 56/100 (777.4s elapsed)\n",
            "Trained classifier 57/100 (782.1s elapsed)\n",
            "Trained classifier 58/100 (786.0s elapsed)\n",
            "Trained classifier 59/100 (791.8s elapsed)\n",
            "Trained classifier 60/100 (795.0s elapsed)\n",
            "Trained classifier 61/100 (799.2s elapsed)\n",
            "Trained classifier 62/100 (806.4s elapsed)\n",
            "Trained classifier 63/100 (809.9s elapsed)\n",
            "Trained classifier 64/100 (813.6s elapsed)\n",
            "Trained classifier 65/100 (817.4s elapsed)\n",
            "Trained classifier 66/100 (826.1s elapsed)\n",
            "Trained classifier 67/100 (829.5s elapsed)\n",
            "Trained classifier 68/100 (834.7s elapsed)\n",
            "Trained classifier 69/100 (840.2s elapsed)\n",
            "Trained classifier 70/100 (845.4s elapsed)\n",
            "Trained classifier 71/100 (852.8s elapsed)\n",
            "Trained classifier 72/100 (861.0s elapsed)\n",
            "Trained classifier 73/100 (866.4s elapsed)\n",
            "Trained classifier 74/100 (871.0s elapsed)\n",
            "Trained classifier 75/100 (876.5s elapsed)\n",
            "Trained classifier 76/100 (881.2s elapsed)\n",
            "Trained classifier 77/100 (885.5s elapsed)\n",
            "Trained classifier 78/100 (890.2s elapsed)\n",
            "Trained classifier 79/100 (895.3s elapsed)\n",
            "Trained classifier 80/100 (901.7s elapsed)\n",
            "Trained classifier 81/100 (905.3s elapsed)\n",
            "Trained classifier 82/100 (910.0s elapsed)\n",
            "Trained classifier 83/100 (915.9s elapsed)\n",
            "Trained classifier 84/100 (919.9s elapsed)\n",
            "Trained classifier 85/100 (926.9s elapsed)\n",
            "Trained classifier 86/100 (930.8s elapsed)\n",
            "Trained classifier 87/100 (938.1s elapsed)\n",
            "Trained classifier 88/100 (944.5s elapsed)\n",
            "Trained classifier 89/100 (948.8s elapsed)\n",
            "Trained classifier 90/100 (955.2s elapsed)\n",
            "Trained classifier 91/100 (961.6s elapsed)\n",
            "Trained classifier 92/100 (965.1s elapsed)\n",
            "Trained classifier 93/100 (969.4s elapsed)\n",
            "Trained classifier 94/100 (974.6s elapsed)\n",
            "Trained classifier 95/100 (978.9s elapsed)\n",
            "Trained classifier 96/100 (984.2s elapsed)\n",
            "Trained classifier 97/100 (989.0s elapsed)\n",
            "Trained classifier 98/100 (997.9s elapsed)\n",
            "Trained classifier 99/100 (1004.5s elapsed)\n",
            "Trained classifier 100/100 (1009.9s elapsed)\n",
            "(15539, 5000)\n",
            "(15539, 256)\n",
            "\n",
            "--- Starting NMF-GT Model Training ---\n",
            "\n",
            "Selecting best column sparsity 'c' from [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]...\n",
            "Calculating YYT (may take time)...\n",
            "YYT calculation took 0.48s. Shape: (256, 256)\n",
            "Performing NMF on YY^T (n_components=100)...\n",
            "NMF completed in 0.06s. Basis shape: (256, 100), Rec. Error: 43791.3745\n",
            "\n",
            "  Testing c = 5...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=5...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 1260, avg col sparsity 4.92\n",
            "Avg Hamming Loss (on 500 samples): 0.281352\n",
            "\n",
            "  Testing c = 10...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=10...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 2556, avg col sparsity 9.98\n",
            "Avg Hamming Loss (on 500 samples): 0.284516\n",
            "\n",
            "  Testing c = 15...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=15...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 3790, avg col sparsity 14.80\n",
            "Avg Hamming Loss (on 500 samples): 0.288367\n",
            "\n",
            "  Testing c = 20...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=20...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 5201, avg col sparsity 20.32\n",
            "Avg Hamming Loss (on 500 samples): 0.284867\n",
            "\n",
            "  Testing c = 25...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=25...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 6362, avg col sparsity 24.85\n",
            "Avg Hamming Loss (on 500 samples): 0.287242\n",
            "\n",
            "  Testing c = 30...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=30...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 7803, avg col sparsity 30.48\n",
            "Avg Hamming Loss (on 500 samples): 0.285250\n",
            "\n",
            "  Testing c = 35...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=35...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 8922, avg col sparsity 34.85\n",
            "Avg Hamming Loss (on 500 samples): 0.278367\n",
            "\n",
            "  Testing c = 40...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=40...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 10308, avg col sparsity 40.27\n",
            "Avg Hamming Loss (on 500 samples): 0.285930\n",
            "\n",
            "  Testing c = 45...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=45...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 11609, avg col sparsity 45.35\n",
            "Avg Hamming Loss (on 500 samples): 0.285766\n",
            "\n",
            "  Testing c = 50...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=50...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 12796, avg col sparsity 49.98\n",
            "Avg Hamming Loss (on 500 samples): 0.273570\n",
            "\n",
            "  Testing c = 55...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=55...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 14191, avg col sparsity 55.43\n",
            "Avg Hamming Loss (on 500 samples): 0.282352\n",
            "\n",
            "  Testing c = 60...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=60...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 15298, avg col sparsity 59.76\n",
            "Avg Hamming Loss (on 500 samples): 0.282039\n",
            "\n",
            "  Testing c = 65...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=65...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 16665, avg col sparsity 65.10\n",
            "Avg Hamming Loss (on 500 samples): 0.275047\n",
            "\n",
            "  Testing c = 70...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=70...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 17990, avg col sparsity 70.27\n",
            "Avg Hamming Loss (on 500 samples): 0.282758\n",
            "\n",
            "  Testing c = 75...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=75...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 19183, avg col sparsity 74.93\n",
            "Avg Hamming Loss (on 500 samples): 0.282313\n",
            "\n",
            "  Testing c = 80...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=80...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 20467, avg col sparsity 79.95\n",
            "Avg Hamming Loss (on 500 samples): 0.279797\n",
            "\n",
            "  Testing c = 85...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=85...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 21861, avg col sparsity 85.39\n",
            "Avg Hamming Loss (on 500 samples): 0.279414\n",
            "\n",
            "  Testing c = 90...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=90...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 22987, avg col sparsity 89.79\n",
            "Avg Hamming Loss (on 500 samples): 0.286445\n",
            "\n",
            "  Testing c = 95...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=95...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 24329, avg col sparsity 95.04\n",
            "Avg Hamming Loss (on 500 samples): 0.277805\n",
            "\n",
            "Selected c = 50 with min Hamming Loss = 0.273570\n",
            "(15539, 256)\n",
            "(15539, 5000)\n",
            "Trained classifier 1/100 (1021.9s elapsed)\n",
            "Trained classifier 2/100 (1027.9s elapsed)\n",
            "Trained classifier 3/100 (1034.5s elapsed)\n",
            "Trained classifier 4/100 (1037.2s elapsed)\n",
            "Trained classifier 5/100 (1041.8s elapsed)\n",
            "Trained classifier 6/100 (1044.8s elapsed)\n",
            "Trained classifier 7/100 (1049.9s elapsed)\n",
            "Trained classifier 8/100 (1053.3s elapsed)\n",
            "Trained classifier 9/100 (1058.7s elapsed)\n",
            "Trained classifier 10/100 (1062.1s elapsed)\n",
            "Trained classifier 11/100 (1066.9s elapsed)\n",
            "Trained classifier 12/100 (1073.7s elapsed)\n",
            "Trained classifier 13/100 (1078.0s elapsed)\n",
            "Trained classifier 14/100 (1081.9s elapsed)\n",
            "Trained classifier 15/100 (1085.5s elapsed)\n",
            "Trained classifier 16/100 (1090.2s elapsed)\n",
            "Trained classifier 17/100 (1094.5s elapsed)\n",
            "Trained classifier 18/100 (1103.1s elapsed)\n",
            "Trained classifier 19/100 (1109.8s elapsed)\n",
            "Trained classifier 20/100 (1115.4s elapsed)\n",
            "Trained classifier 21/100 (1119.7s elapsed)\n",
            "Trained classifier 22/100 (1127.1s elapsed)\n",
            "Trained classifier 23/100 (1132.1s elapsed)\n",
            "Trained classifier 24/100 (1138.1s elapsed)\n",
            "Trained classifier 25/100 (1142.3s elapsed)\n",
            "Trained classifier 26/100 (1149.1s elapsed)\n",
            "Trained classifier 27/100 (1156.0s elapsed)\n",
            "Trained classifier 28/100 (1162.6s elapsed)\n",
            "Trained classifier 29/100 (1169.1s elapsed)\n",
            "Trained classifier 30/100 (1175.1s elapsed)\n",
            "Trained classifier 31/100 (1179.5s elapsed)\n",
            "Trained classifier 32/100 (1183.4s elapsed)\n",
            "Trained classifier 33/100 (1190.5s elapsed)\n",
            "Trained classifier 34/100 (1195.3s elapsed)\n",
            "Trained classifier 35/100 (1199.6s elapsed)\n",
            "Trained classifier 36/100 (1203.3s elapsed)\n",
            "Trained classifier 37/100 (1207.0s elapsed)\n",
            "Trained classifier 38/100 (1210.4s elapsed)\n",
            "Trained classifier 39/100 (1214.2s elapsed)\n",
            "Trained classifier 40/100 (1219.3s elapsed)\n",
            "Trained classifier 41/100 (1224.0s elapsed)\n",
            "Trained classifier 42/100 (1227.1s elapsed)\n",
            "Trained classifier 43/100 (1232.5s elapsed)\n",
            "Trained classifier 44/100 (1238.6s elapsed)\n",
            "Trained classifier 45/100 (1245.8s elapsed)\n",
            "Trained classifier 46/100 (1250.6s elapsed)\n",
            "Trained classifier 47/100 (1256.0s elapsed)\n",
            "Trained classifier 48/100 (1259.3s elapsed)\n",
            "Trained classifier 49/100 (1265.2s elapsed)\n",
            "Trained classifier 50/100 (1269.3s elapsed)\n",
            "Trained classifier 51/100 (1273.7s elapsed)\n",
            "Trained classifier 52/100 (1279.2s elapsed)\n",
            "Trained classifier 53/100 (1282.9s elapsed)\n",
            "Trained classifier 54/100 (1290.4s elapsed)\n",
            "Trained classifier 55/100 (1293.2s elapsed)\n",
            "Trained classifier 56/100 (1296.6s elapsed)\n",
            "Trained classifier 57/100 (1301.8s elapsed)\n",
            "Trained classifier 58/100 (1305.1s elapsed)\n",
            "Trained classifier 59/100 (1311.0s elapsed)\n",
            "Trained classifier 60/100 (1316.3s elapsed)\n",
            "Trained classifier 61/100 (1321.4s elapsed)\n",
            "Trained classifier 62/100 (1324.7s elapsed)\n",
            "Trained classifier 63/100 (1329.8s elapsed)\n",
            "Trained classifier 64/100 (1334.9s elapsed)\n",
            "Trained classifier 65/100 (1339.6s elapsed)\n",
            "Trained classifier 66/100 (1344.3s elapsed)\n",
            "Trained classifier 67/100 (1348.2s elapsed)\n",
            "Trained classifier 68/100 (1351.1s elapsed)\n",
            "Trained classifier 69/100 (1356.9s elapsed)\n",
            "Trained classifier 70/100 (1363.1s elapsed)\n",
            "Trained classifier 71/100 (1370.0s elapsed)\n",
            "Trained classifier 72/100 (1377.3s elapsed)\n",
            "Trained classifier 73/100 (1382.3s elapsed)\n",
            "Trained classifier 74/100 (1386.4s elapsed)\n",
            "Trained classifier 75/100 (1390.1s elapsed)\n",
            "Trained classifier 76/100 (1395.0s elapsed)\n",
            "Trained classifier 77/100 (1401.3s elapsed)\n",
            "Trained classifier 78/100 (1406.9s elapsed)\n",
            "Trained classifier 79/100 (1411.5s elapsed)\n",
            "Trained classifier 80/100 (1416.3s elapsed)\n",
            "Trained classifier 81/100 (1423.1s elapsed)\n",
            "Trained classifier 82/100 (1428.6s elapsed)\n",
            "Trained classifier 83/100 (1432.2s elapsed)\n",
            "Trained classifier 84/100 (1440.7s elapsed)\n",
            "Trained classifier 85/100 (1445.9s elapsed)\n",
            "Trained classifier 86/100 (1452.8s elapsed)\n",
            "Trained classifier 87/100 (1457.8s elapsed)\n",
            "Trained classifier 88/100 (1463.7s elapsed)\n",
            "Trained classifier 89/100 (1466.9s elapsed)\n",
            "Trained classifier 90/100 (1472.3s elapsed)\n",
            "Trained classifier 91/100 (1476.6s elapsed)\n",
            "Trained classifier 92/100 (1481.6s elapsed)\n",
            "Trained classifier 93/100 (1485.5s elapsed)\n",
            "Trained classifier 94/100 (1489.2s elapsed)\n",
            "Trained classifier 95/100 (1493.6s elapsed)\n",
            "Trained classifier 96/100 (1497.7s elapsed)\n",
            "Trained classifier 97/100 (1501.6s elapsed)\n",
            "Trained classifier 98/100 (1507.8s elapsed)\n",
            "Trained classifier 99/100 (1511.4s elapsed)\n",
            "Trained classifier 100/100 (1514.8s elapsed)\n",
            "(15539, 5000)\n",
            "(15539, 256)\n",
            "\n",
            "--- Starting NMF-GT Model Training ---\n",
            "\n",
            "Selecting best column sparsity 'c' from [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]...\n",
            "Calculating YYT (may take time)...\n",
            "YYT calculation took 0.60s. Shape: (256, 256)\n",
            "Performing NMF on YY^T (n_components=100)...\n",
            "NMF completed in 0.20s. Basis shape: (256, 100), Rec. Error: 46905.7720\n",
            "\n",
            "  Testing c = 5...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=5...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 1251, avg col sparsity 4.89\n",
            "Avg Hamming Loss (on 500 samples): 0.297242\n",
            "\n",
            "  Testing c = 10...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=10...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 2535, avg col sparsity 9.90\n",
            "Avg Hamming Loss (on 500 samples): 0.305984\n",
            "\n",
            "  Testing c = 15...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=15...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 3793, avg col sparsity 14.82\n",
            "Avg Hamming Loss (on 500 samples): 0.304922\n",
            "\n",
            "  Testing c = 20...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=20...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 5104, avg col sparsity 19.94\n",
            "Avg Hamming Loss (on 500 samples): 0.310594\n",
            "\n",
            "  Testing c = 25...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=25...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 6355, avg col sparsity 24.82\n",
            "Avg Hamming Loss (on 500 samples): 0.294016\n",
            "\n",
            "  Testing c = 30...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=30...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 7601, avg col sparsity 29.69\n",
            "Avg Hamming Loss (on 500 samples): 0.306516\n",
            "\n",
            "  Testing c = 35...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=35...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 8844, avg col sparsity 34.55\n",
            "Avg Hamming Loss (on 500 samples): 0.299117\n",
            "\n",
            "  Testing c = 40...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=40...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 10208, avg col sparsity 39.88\n",
            "Avg Hamming Loss (on 500 samples): 0.295359\n",
            "\n",
            "  Testing c = 45...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=45...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 11540, avg col sparsity 45.08\n",
            "Avg Hamming Loss (on 500 samples): 0.303492\n",
            "\n",
            "  Testing c = 50...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=50...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 12836, avg col sparsity 50.14\n",
            "Avg Hamming Loss (on 500 samples): 0.297773\n",
            "\n",
            "  Testing c = 55...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=55...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 14066, avg col sparsity 54.95\n",
            "Avg Hamming Loss (on 500 samples): 0.302180\n",
            "\n",
            "  Testing c = 60...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=60...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 15441, avg col sparsity 60.32\n",
            "Avg Hamming Loss (on 500 samples): 0.303453\n",
            "\n",
            "  Testing c = 65...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=65...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 16570, avg col sparsity 64.73\n",
            "Avg Hamming Loss (on 500 samples): 0.296750\n",
            "\n",
            "  Testing c = 70...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=70...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 17911, avg col sparsity 69.96\n",
            "Avg Hamming Loss (on 500 samples): 0.301438\n",
            "\n",
            "  Testing c = 75...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=75...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 19272, avg col sparsity 75.28\n",
            "Avg Hamming Loss (on 500 samples): 0.297836\n",
            "\n",
            "  Testing c = 80...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=80...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 20379, avg col sparsity 79.61\n",
            "Avg Hamming Loss (on 500 samples): 0.291109\n",
            "\n",
            "  Testing c = 85...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=85...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 21764, avg col sparsity 85.02\n",
            "Avg Hamming Loss (on 500 samples): 0.308375\n",
            "\n",
            "  Testing c = 90...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=90...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 23019, avg col sparsity 89.92\n",
            "Avg Hamming Loss (on 500 samples): 0.301563\n",
            "\n",
            "  Testing c = 95...\n",
            "Generating GT matrix A (100 x 256) with target column sparsity c=95...\n",
            "Generated GT column 50/256\n",
            "Generated GT column 100/256\n",
            "Generated GT column 150/256\n",
            "Generated GT column 200/256\n",
            "Generated GT column 250/256\n",
            "Generated GT column 256/256\n",
            "Generated GT matrix A: shape (100, 256), nnz 24290, avg col sparsity 94.88\n",
            "Avg Hamming Loss (on 500 samples): 0.302938\n",
            "\n",
            "Selected c = 80 with min Hamming Loss = 0.291109\n",
            "(15539, 256)\n",
            "(15539, 5000)\n",
            "Trained classifier 1/100 (1522.9s elapsed)\n",
            "Trained classifier 2/100 (1527.2s elapsed)\n",
            "Trained classifier 3/100 (1532.2s elapsed)\n",
            "Trained classifier 4/100 (1537.1s elapsed)\n",
            "Trained classifier 5/100 (1541.6s elapsed)\n",
            "Trained classifier 6/100 (1547.3s elapsed)\n",
            "Trained classifier 7/100 (1549.9s elapsed)\n",
            "Trained classifier 8/100 (1554.6s elapsed)\n",
            "Trained classifier 9/100 (1557.8s elapsed)\n",
            "Trained classifier 10/100 (1561.8s elapsed)\n",
            "Trained classifier 11/100 (1565.2s elapsed)\n",
            "Trained classifier 12/100 (1573.0s elapsed)\n",
            "Trained classifier 13/100 (1577.9s elapsed)\n",
            "Trained classifier 14/100 (1582.0s elapsed)\n",
            "Trained classifier 15/100 (1588.2s elapsed)\n",
            "Trained classifier 16/100 (1593.7s elapsed)\n",
            "Trained classifier 17/100 (1600.1s elapsed)\n",
            "Trained classifier 18/100 (1606.4s elapsed)\n",
            "Trained classifier 19/100 (1611.7s elapsed)\n",
            "Trained classifier 20/100 (1615.2s elapsed)\n",
            "Trained classifier 21/100 (1619.3s elapsed)\n",
            "Trained classifier 22/100 (1622.5s elapsed)\n",
            "Trained classifier 23/100 (1627.8s elapsed)\n",
            "Trained classifier 24/100 (1634.8s elapsed)\n",
            "Trained classifier 25/100 (1637.5s elapsed)\n",
            "Trained classifier 26/100 (1641.5s elapsed)\n",
            "Trained classifier 27/100 (1644.9s elapsed)\n",
            "Trained classifier 28/100 (1649.4s elapsed)\n",
            "Trained classifier 29/100 (1652.1s elapsed)\n",
            "Trained classifier 30/100 (1656.3s elapsed)\n",
            "Trained classifier 31/100 (1660.3s elapsed)\n",
            "Trained classifier 32/100 (1667.6s elapsed)\n",
            "Trained classifier 33/100 (1671.0s elapsed)\n",
            "Trained classifier 34/100 (1673.8s elapsed)\n",
            "Trained classifier 35/100 (1677.7s elapsed)\n",
            "Trained classifier 36/100 (1683.4s elapsed)\n",
            "Trained classifier 37/100 (1687.7s elapsed)\n",
            "Trained classifier 38/100 (1691.9s elapsed)\n",
            "Trained classifier 39/100 (1695.0s elapsed)\n",
            "Trained classifier 40/100 (1700.8s elapsed)\n",
            "Trained classifier 41/100 (1705.2s elapsed)\n",
            "Trained classifier 42/100 (1710.8s elapsed)\n",
            "Trained classifier 43/100 (1718.6s elapsed)\n",
            "Trained classifier 44/100 (1724.6s elapsed)\n",
            "Trained classifier 45/100 (1730.5s elapsed)\n",
            "Trained classifier 46/100 (1733.6s elapsed)\n",
            "Trained classifier 47/100 (1736.7s elapsed)\n",
            "Trained classifier 48/100 (1741.9s elapsed)\n",
            "Trained classifier 49/100 (1745.6s elapsed)\n",
            "Trained classifier 50/100 (1749.2s elapsed)\n",
            "Trained classifier 51/100 (1753.6s elapsed)\n",
            "Trained classifier 52/100 (1756.6s elapsed)\n",
            "Trained classifier 53/100 (1759.2s elapsed)\n",
            "Trained classifier 54/100 (1762.9s elapsed)\n",
            "Trained classifier 55/100 (1766.8s elapsed)\n",
            "Trained classifier 56/100 (1771.3s elapsed)\n",
            "Trained classifier 57/100 (1774.4s elapsed)\n",
            "Trained classifier 58/100 (1777.7s elapsed)\n",
            "Trained classifier 59/100 (1782.4s elapsed)\n",
            "Trained classifier 60/100 (1791.4s elapsed)\n",
            "Trained classifier 61/100 (1794.9s elapsed)\n",
            "Trained classifier 62/100 (1800.0s elapsed)\n",
            "Trained classifier 63/100 (1804.6s elapsed)\n",
            "Trained classifier 64/100 (1808.9s elapsed)\n",
            "Trained classifier 65/100 (1816.0s elapsed)\n",
            "Trained classifier 66/100 (1821.3s elapsed)\n",
            "Trained classifier 67/100 (1828.3s elapsed)\n",
            "Trained classifier 68/100 (1835.3s elapsed)\n",
            "Trained classifier 69/100 (1838.6s elapsed)\n",
            "Trained classifier 70/100 (1842.3s elapsed)\n",
            "Trained classifier 71/100 (1844.3s elapsed)\n",
            "Trained classifier 72/100 (1850.1s elapsed)\n",
            "Trained classifier 73/100 (1853.8s elapsed)\n",
            "Trained classifier 74/100 (1856.8s elapsed)\n",
            "Trained classifier 75/100 (1861.0s elapsed)\n",
            "Trained classifier 76/100 (1864.5s elapsed)\n",
            "Trained classifier 77/100 (1868.2s elapsed)\n",
            "Trained classifier 78/100 (1873.6s elapsed)\n",
            "Trained classifier 79/100 (1877.2s elapsed)\n",
            "Trained classifier 80/100 (1882.4s elapsed)\n",
            "Trained classifier 81/100 (1885.9s elapsed)\n",
            "Trained classifier 82/100 (1889.6s elapsed)\n",
            "Trained classifier 83/100 (1896.3s elapsed)\n",
            "Trained classifier 84/100 (1901.7s elapsed)\n",
            "Trained classifier 85/100 (1907.5s elapsed)\n",
            "Trained classifier 86/100 (1915.5s elapsed)\n",
            "Trained classifier 87/100 (1919.9s elapsed)\n",
            "Trained classifier 88/100 (1923.6s elapsed)\n",
            "Trained classifier 89/100 (1927.4s elapsed)\n",
            "Trained classifier 90/100 (1933.4s elapsed)\n",
            "Trained classifier 91/100 (1938.3s elapsed)\n",
            "Trained classifier 92/100 (1941.7s elapsed)\n",
            "Trained classifier 93/100 (1946.4s elapsed)\n",
            "Trained classifier 94/100 (1948.6s elapsed)\n",
            "Trained classifier 95/100 (1952.3s elapsed)\n",
            "Trained classifier 96/100 (1955.5s elapsed)\n",
            "Trained classifier 97/100 (1959.4s elapsed)\n",
            "Trained classifier 98/100 (1964.1s elapsed)\n",
            "Trained classifier 99/100 (1968.1s elapsed)\n",
            "Trained classifier 100/100 (1973.2s elapsed)\n"
          ]
        }
      ],
      "source": [
        "start_clf_time = time.time()\n",
        "SVMs= []\n",
        "A1s = []\n",
        "for i in range(num_partitions):\n",
        "    Y1 = Yp[i]\n",
        "    Y1 = Y1.transpose()\n",
        "    X1 = X_train\n",
        "    nmfgt_model = NMFGTModel(\n",
        "        n_groups=n_groups_m,\n",
        "        k_target_sparsity=k_target,\n",
        "        column_sparsity_range=c_range,\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    A1 = nmfgt_model.fit(X1, Y1)\n",
        "    A1s.append(A1)\n",
        "    Z = (A1 @ Y1.T > 0).astype(int)\n",
        "    SVM = []\n",
        "    m=Z.shape[0]\n",
        "    Z = Z.toarray()\n",
        "    X1 = X1.toarray()\n",
        "    for j in range(m):\n",
        "        y2 = Z[j, :]\n",
        "        unique_classes = np.unique(y2)\n",
        "        if len(unique_classes) < 2:\n",
        "            print(f\"Label {i} has only one class: {unique_classes[0]}\")\n",
        "            clf = DummyClassifier(strategy=\"constant\", constant=unique_classes[0])\n",
        "            clf.fit(X1, y2)\n",
        "        else:\n",
        "            clf = SGDClassifier(loss='log_loss')\n",
        "            clf.fit(X1, y2)\n",
        "        SVM.append(clf)\n",
        "        elapsed_time = time.time() - start_clf_time\n",
        "        print(f\"Trained classifier {j+1}/{m} ({elapsed_time:.1f}s elapsed)\")\n",
        "    SVMs.append(SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3b88e1",
      "metadata": {},
      "source": [
        "### Calculate the Z's for the X_test and convert them back to the Y matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "5a957d0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/pg24/manivannan/miniconda3/envs/AML_HW5/lib/python3.9/site-packages/scipy/sparse/_index.py:151: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n",
            "/users/pg24/manivannan/miniconda3/envs/AML_HW5/lib/python3.9/site-packages/scipy/sparse/_index.py:142: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray_sparse(i, j, x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "ATp = csr_matrix((Y_train.shape[0], X_test.shape[0]))\n",
        "for i in range(num_partitions):\n",
        "    Ztest = csr_matrix((n_groups_m, X_test.shape[0]))\n",
        "    print(i)\n",
        "    for j in range(m):\n",
        "        z_pred = SVMs[i][j].predict(X_test)\n",
        "        Ztest[j,:] = z_pred.T\n",
        "    test_scores=A1s[i].T@Ztest\n",
        "    tmp_ATp = csr_matrix((Y_train.shape[0], X_test.shape[0]))\n",
        "    tmp_ATp[set_list[i],:] =  test_scores\n",
        "    ATp = ATp + tmp_ATp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31864892",
      "metadata": {},
      "source": [
        "### decoding the predicted data back to the original dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "e56e5e54",
      "metadata": {},
      "outputs": [],
      "source": [
        "ATp_decoded = model.decoder(torch.tensor(ATp.T.toarray(), dtype=torch.float32).to('cuda')).cpu().detach().numpy()\n",
        "ATp_decoded = csr_matrix(ATp_decoded).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2235157",
      "metadata": {},
      "source": [
        "### Calculate the precision @ K problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "716729ee",
      "metadata": {
        "id": "716729ee"
      },
      "outputs": [],
      "source": [
        "def precision_k(score_mat, true_mat, K):\n",
        "    return _helper(score_mat, true_mat, K)\n",
        "\n",
        "def _helper(score_mat, true_mat, K):\n",
        "    num_lbl, num_inst = score_mat.shape\n",
        "    P = np.zeros(K)\n",
        "\n",
        "    score_mat = csr_matrix(score_mat)\n",
        "\n",
        "    rank_mat = sort_sparse_mat(score_mat)\n",
        "\n",
        "    for k in range(1, K + 1):\n",
        "        mat = rank_mat.copy()\n",
        "        mat.data[mat.data > k] = 0\n",
        "        mat.eliminate_zeros()\n",
        "\n",
        "        mat.data = np.ones_like(mat.data)\n",
        "\n",
        "        mat = mat.multiply(true_mat)\n",
        "\n",
        "        num = np.array(mat.sum(axis=0)).flatten()\n",
        "\n",
        "        P[k - 1] = np.mean(num / k)\n",
        "\n",
        "    return P\n",
        "\n",
        "def sort_sparse_mat(score_mat):\n",
        "    \"\"\"Returns a matrix where each column has rank values for non-zero elements.\"\"\"\n",
        "    score_mat = score_mat.tocsc()\n",
        "    rank_mat = lil_matrix(score_mat.shape)\n",
        "\n",
        "    for j in range(score_mat.shape[1]):\n",
        "        col = score_mat[:, j].toarray().flatten()\n",
        "        if np.count_nonzero(col) == 0:\n",
        "            continue\n",
        "        ranked_indices = np.argsort(-col)\n",
        "        ranks = np.zeros_like(col)\n",
        "        ranks[ranked_indices] = np.arange(1, len(col) + 1)\n",
        "        ranks[col == 0] = 0\n",
        "        rank_mat[:, j] = ranks.reshape(-1, 1)\n",
        "\n",
        "    return rank_mat.tocsr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294b5a9e",
      "metadata": {},
      "source": [
        "### decoding the original data back to the original dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "ea2cb849",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<3993x3809 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 6365972 stored elements in Compressed Sparse Column format>"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test_decoded = model.decoder(torch.tensor(Y_test_perm.T.toarray(), dtype=torch.float32).to('cuda')).cpu().detach().numpy()\n",
        "y_test_decoded = csr_matrix(y_test_decoded).T\n",
        "y_test_decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "8dbc5a41",
      "metadata": {
        "id": "8dbc5a41"
      },
      "outputs": [],
      "source": [
        "P_old = precision_k(ATp_decoded, y_test_decoded, K_EVAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "488cc46c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "488cc46c",
        "outputId": "c3b5432f-8ce5-4d30-e310-083c14c4e2d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.96010691, 0.9611735 , 0.9616904 , 0.94265091, 0.94212378])"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "P_old"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0df8d177",
      "metadata": {},
      "source": [
        "### Calculate the modified precision @ K problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "4728cdd9",
      "metadata": {
        "id": "4728cdd9"
      },
      "outputs": [],
      "source": [
        "def precision_k_new(score_mat, true_mat, K):\n",
        "    return helper(score_mat, true_mat, K)\n",
        "\n",
        "def helper(score_mat, true_mat, K):\n",
        "    num_lbl, num_inst = score_mat.shape\n",
        "    P = np.zeros(K)\n",
        "\n",
        "    rank_mat = sort_sparse_mat(score_mat)\n",
        "\n",
        "    mat = []\n",
        "    for j in range(num_inst):\n",
        "        tmp = rank_mat[:, j].copy()\n",
        "        tmp[tmp > K] = 0\n",
        "        mat_col = (tmp > 0).astype(int)\n",
        "        mat.append(mat_col)\n",
        "    mat = np.stack(mat, axis=1)\n",
        "    mat = csr_matrix(mat)\n",
        "\n",
        "    mat = mat.multiply(true_mat)\n",
        "    num = np.array(mat.sum(axis=0)).flatten()\n",
        "\n",
        "    for k in range(1, K + 1):\n",
        "        num2 = np.minimum(num, k)\n",
        "        P[k - 1] = np.mean(num2 / k)\n",
        "\n",
        "    return P\n",
        "\n",
        "def sort_sparse_mat(score_mat):\n",
        "    \"\"\"\n",
        "    For each column, return a matrix of ranks.\n",
        "    Higher scores get lower rank numbers (1 = highest).\n",
        "    \"\"\"\n",
        "    num_lbl, num_inst = score_mat.shape\n",
        "    rank_mat = np.zeros((num_lbl, num_inst), dtype=int)\n",
        "\n",
        "    score_mat = score_mat.toarray()\n",
        "\n",
        "    for j in range(num_inst):\n",
        "        scores = score_mat[:, j]\n",
        "        order = np.argsort(-scores)\n",
        "        ranks = np.empty_like(order)\n",
        "        ranks[order] = np.arange(1, num_lbl + 1)\n",
        "        rank_mat[:, j] = ranks\n",
        "\n",
        "    return rank_mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "52877367",
      "metadata": {
        "id": "52877367"
      },
      "outputs": [],
      "source": [
        "P = precision_k_new(ATp_decoded, y_test_decoded, K_EVAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "968df05e",
      "metadata": {
        "id": "968df05e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.        , 1.        , 1.        , 0.98302984, 0.94212378])"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "36eed346",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "ks = [1,2,3,4,5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4a4306",
      "metadata": {},
      "source": [
        "### Plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "a3575061",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(ks, P_old, 'r*-', label='Precision')\n",
        "plt.plot(ks, P, 'g*-', label='Precision_modified')\n",
        "\n",
        "plt.title('Precision Eurlex Test data')\n",
        "plt.ylabel('Precision@k')\n",
        "plt.xlabel('k')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af185ba0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
